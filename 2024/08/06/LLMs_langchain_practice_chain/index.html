<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    <meta property="og:site_name" content="YuTian Blog">
    <meta property="og:type" content="article">

    
    <meta property="og:image" content="/img/2024-08-06-LLMs_langchain_practice_chain/langchain.png">
    <meta property="twitter:image" content="/img/2024-08-06-LLMs_langchain_practice_chain/langchain.png" />
    

    
    <meta name="title" content="langchain开发框架实践（一）" />
    <meta property="og:title" content="langchain开发框架实践（一）" />
    <meta property="twitter:title" content="langchain开发框架实践（一）" />
    

    
    <meta name="description" content="介绍了如何使用langchain这套开发框架开发智能体应用">
    <meta property="og:description" content="介绍了如何使用langchain这套开发框架开发智能体应用" />
    <meta property="twitter:description" content="介绍了如何使用langchain这套开发框架开发智能体应用" />
    

    
    <meta property="twitter:card" content="summary" />
    
    

    <meta name="keyword"  content="虞天, 虞天的博客, YuTian Blog, 博客, llms, 大模型, Kubernetes, 微服务, ">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>langchain开发框架实践（一） | 虞天的博客 | YuTian Blog</title>

    <link rel="canonical" href="/2024/08/06/LLMs_langchain_practice_chain/">

    
    
    
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    
    <link rel="stylesheet" href="/css/hugo-theme-cleanwhite.min.css">

    
    <link rel="stylesheet" href="/css/zanshang.css">

    
    <link rel="stylesheet" href="/css/font-awesome.all.min.css">

    
    

    
    <script src="/js/jquery.min.js"></script>

    
    <script src="/js/bootstrap.min.js"></script>

    
    <script src="/js/hux-blog.min.js"></script>

    
    <script src="/js/lazysizes.min.js"></script>

    
    

</head>






<nav class="navbar navbar-default navbar-custom navbar-fixed-top">

    <div class="container-fluid">
        
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">YuTian Blog</a>
        </div>

        
        
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">All Posts</a>
                    </li>
                    
                        
                        <li>
                            <a href="/categories/life/">life</a>
                        </li>
                        
                        <li>
                            <a href="/categories/tech/">tech</a>
                        </li>
                        
                    
                    
		    
                        <li><a href="/archive//">ARCHIVE</a></li>
                    
                        <li><a href="/notes//">NOTES</a></li>
                    
                        <li><a href="/about//">ABOUT</a></li>
                    
		            <li>
                        <a href="/search"><i class="fa fa-search"></i></a>
		           </li>
                </ul>
            </div>
        </div>
        
    </div>
    
</nav>
<script>
    
    
    
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        
            $navbar.className = " ";
            
            setTimeout(function(){
                
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>




<style type="text/css">
    header.intro-header {
        background-image: url('/img/2024-08-06-LLMs_langchain_practice_chain/langchain.png')
    }
</style>

<header class="intro-header" >

    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/llms-dev_framework" title="LLMs, dev_framework">
                            LLMs, dev_framework
                        </a>
                        
                    </div>
                    <h1>langchain开发框架实践（一）</h1>
                    <h2 class="subheading"></h2>
                    <span class="meta">
                        
                            Posted by 
                            
                                    &#34;虞天&#34;
                             
                            on 
                            Tuesday, August 6, 2024
                            
                            
                            
                            
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>




<article>
    <div class="container">
        <div class="row">

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                
                <h2 id="1langchain是什么">1、langchain是什么</h2>
<p>LangChain是一个基于LLM开发应用程序的框架，把调用LLM的过程组成一条链的形式，具体要执行哪些函数是由LLM的推理结果决定的。（区别于传统程序是写死的）同时LangChain也是一个丰富的工具生态系统的一部分，我们可以在此框架集成并在其之上构建自己的Agent。</p>
<p>LangChain的模块组成：</p>
<ul>
<li>Model I/O（与语言模型进行接口）</li>
<li>Retriever（与特定于应用程序的数据进行接口）</li>
<li>Memory（在Pipeline运行期间保持记忆状态）</li>
<li>Chain（构建调用序列链条）</li>
<li>Agent（让管道根据高级指令选择使用哪些工具）</li>
<li>Callback（记录和流式传输任何管道的中间步骤）</li>
</ul>
<h2 id="2模块-chain">2、模块 Chain</h2>
<p>LangChain应用程序的核心构建模块是LLMChain。它结合了三个方面:</p>
<ul>
<li>LLM: 语言模型是核心推理引擎。要使用LangChain，您需要了解不同类型的语言模型以及如何使用它们。</li>
<li>Prompt Templates: 提供语言模型的指令。这控制了语言模型的输出，因此了解如何构建提示和不同的提示策略至关重要。</li>
<li>Output Parsers: 将LLM的原始响应转换为更易处理的格式，使得在下游使用输出变得容易。</li>
</ul>
<p>每个Langchain组件都是LCEL对象，我们可以使用LangChain 表达式语句（LCEL）轻松的将各个组件链接在一起，如下实现prompt + model + output parser的chain = prompt | llm | output_parser，其中| 符号可以实现将数据从一个组件提供的输出，输入到下一个组件中：</p>
<p>我们结合代码进行具体的说明</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain_core.prompts <span style="color:#ff79c6">import</span> ChatPromptTemplate
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain_core.output_parsers <span style="color:#ff79c6">import</span> StrOutputParser
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain_openai <span style="color:#ff79c6">import</span> ChatOpenAI
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 定义提示模板</span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_messages([
</span></span><span style="display:flex;"><span>    (<span style="color:#f1fa8c">&#34;system&#34;</span>, <span style="color:#f1fa8c">&#34;You are a world-class technical documentation writer.&#34;</span>),
</span></span><span style="display:flex;"><span>    (<span style="color:#f1fa8c">&#34;user&#34;</span>, <span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">{input}</span><span style="color:#f1fa8c">&#34;</span>)
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 填充模型名</span>
</span></span><span style="display:flex;"><span>llm <span style="color:#ff79c6">=</span> ChatOpenAI(
</span></span><span style="display:flex;"><span>    model_name<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;qwen1.5-32b-chat-int4&#39;</span>,
</span></span><span style="display:flex;"><span>    openai_api_base<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;http://20.20.136.251:8001/v1&#39;</span>,
</span></span><span style="display:flex;"><span>    openai_api_key<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;q7r8s9t0-u1v2-w3x4-y5z6-a7b8c9d0e1f2&#39;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 定义输出解析器</span>
</span></span><span style="display:flex;"><span>output_parser <span style="color:#ff79c6">=</span> StrOutputParser()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 创建链条</span>
</span></span><span style="display:flex;"><span>chain <span style="color:#ff79c6">=</span> prompt <span style="color:#ff79c6">|</span> llm <span style="color:#ff79c6">|</span> output_parser
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 生成 prompt 的值</span>
</span></span><span style="display:flex;"><span>prompt_value <span style="color:#ff79c6">=</span> prompt<span style="color:#ff79c6">.</span>invoke({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;how can langsmith help with testing?&#34;</span>})
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(prompt_value)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 通过链条获取模型的响应</span>
</span></span><span style="display:flex;"><span>response <span style="color:#ff79c6">=</span> chain<span style="color:#ff79c6">.</span>invoke(prompt_value)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 打印模型响应</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(response)
</span></span></code></pre></div><p>接下来仔细看一些下着三个组件的输入和输出：</p>
<p>prompt</p>
<ul>
<li>prompt是一个 ChatPromptTemplate，这意味着它接受模板变量的字典并生成 PromptValue。PromptValue是完整提示的包装器，可以传递给 LLM （将字符串作为输入）或ChatModel（将一系列消息作为输入）。</li>
<li>打印出来可以看到，prompt_value 是一个ChatPromptValue对象，里面的message是一个list，包含不同角色message的对话信息。</li>
</ul>
<p>model</p>
<ul>
<li>如果model为 ChatModel，这意味着它将输出 BaseMessage。而如果我们的model是 LLM，它将输出一个字符串。</li>
</ul>
<p>output_parser</p>
<ul>
<li>最后，我们将model输出传递给output_parser，这意味着 BaseOutputParser它需要字符串或 BaseMessage 作为输入。StrOutputParser是将任何输入转换为字符串。</li>
</ul>
<h2 id="3模块-model-io">3、模块 Model I/O</h2>
<p>首先我们从最基本面的部分讲起，Model I/O 指的是和LLM直接进行交互的过程。</p>
<h4 id="31模块-prompt-templates">3.1、模块 Prompt Templates</h4>
<ul>
<li>Prompt:
<ul>
<li>指用户的一系列指令和输入，是决定Language Model输出内容的唯一输入，主要用于帮助模型理解上下文，并生成相关和连贯的输出，如回答问题、拓写句子和总结问题。</li>
</ul>
</li>
<li>Prompt Template:
<ul>
<li>预定义的一系列指令和输入参数的prompt模版(默认使用str.fromat格式化)，支持更加灵活的输入，如支持output instruction(输出格式指令), partial input(提前指定部分输入参数), examples(输入输出示例)等；LangChain提供了大量方法来创建Prompt Template，有了这一层组件就可以在不同Language Model和不同Chain下大量复用Prompt Template了。</li>
</ul>
</li>
<li>Example selectors:
<ul>
<li>在很多场景下，单纯的instruction + input的prompt不足以让LLM完成高质量的推理回答，这时候我们就还需要为prompt补充一些针对具体问题的示例（in-context learning），LangChain将这一功能抽象为了Example selectors这一组件，我们可以基于关键字，相似度(通常使用MMR/cosine similarity/ngram来计算相似度, 在后面的向量数据库章节中会提到)。为了让最终的prompt不超过Language Model的token上限（各个模型的token上限见下表），LangChain还提供了LengthBasedExampleSelector，根据长度来限制example数量，对于较长的输入，它会选择包含较少示例的提示，而对于较短的输入，它会选择包含更多示例。</li>
</ul>
</li>
</ul>
<p><strong>PromptTemplate</strong>：langchain.prompts中的PromptTemplate类使用from_template可以创建单个字符串类型的prompt，需要str.format()格式化prompt的参数。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> PromptTemplate
</span></span><span style="display:flex;"><span>prompt_template <span style="color:#ff79c6">=</span> PromptTemplate<span style="color:#ff79c6">.</span>from_template(
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;Tell me a </span><span style="color:#f1fa8c">{adjective}</span><span style="color:#f1fa8c"> joke about </span><span style="color:#f1fa8c">{content}</span><span style="color:#f1fa8c">.&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>prompt_template<span style="color:#ff79c6">.</span>format(adjective<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;funny&#34;</span>, content<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;chickens&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># Tell me a funny joke about chickens.</span>
</span></span></code></pre></div><p><strong>ChatPromptTemplate</strong>：langchain_core.prompts中的ChatPromptTemplate类，使用from_messages方法可以创建chat message类型的prompt（包含role=system/human/ai和content=str_text两部分），这样就可以以list形式，构造history对话记录，让llm进行in-context learning。需要format_messages格式化prompt的参数，得到的是AIMessage, HumanMessage, SystemMessage三者的list组合（当然我们也可以自己构建这个list，并实例化对应的Message对象）。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain_core.prompts <span style="color:#ff79c6">import</span> ChatPromptTemplate
</span></span><span style="display:flex;"><span>chat_template <span style="color:#ff79c6">=</span> ChatPromptTemplate<span style="color:#ff79c6">.</span>from_messages(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        (<span style="color:#f1fa8c">&#34;system&#34;</span>, <span style="color:#f1fa8c">&#34;You are a helpful AI bot. Your name is </span><span style="color:#f1fa8c">{name}</span><span style="color:#f1fa8c">.&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#f1fa8c">&#34;human&#34;</span>, <span style="color:#f1fa8c">&#34;Hello, how are you doing?&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#f1fa8c">&#34;ai&#34;</span>, <span style="color:#f1fa8c">&#34;I&#39;m doing well, thanks!&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#f1fa8c">&#34;human&#34;</span>, <span style="color:#f1fa8c">&#34;</span><span style="color:#f1fa8c">{user_input}</span><span style="color:#f1fa8c">&#34;</span>),
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>messages <span style="color:#ff79c6">=</span> chat_template<span style="color:#ff79c6">.</span>format_messages(name<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Bob&#34;</span>, user_input<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;What is your name?&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># [SystemMessage(content=&#34;You are a helpful assistant that re-writes the user&#39;s text to sound more upbeat.&#34;), HumanMessage(content=&#34;I don&#39;t like eating tasty things&#34;)]</span>
</span></span></code></pre></div><p><strong>FewShotPromptTemplate</strong>：当然更加专业的in-context learnding方式是使用FewShotPromptTemplate显示的指定Examples，构造few shot prompt。使用字典来构造examples，每个example都是一个dict，多个examples存储在一个list中。当我们给定了很多examples时，这需要使用Example Selector利用MMR/cosine similarity/ngram来计算input和examples之间的语义相似度，来决定选择哪些examples。</p>
<p>如下给出反义词的task（使用基于example长度的LengthBasedExampleSelector）：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#6272a4"># 1. 定义example variables list</span>
</span></span><span style="display:flex;"><span>examples <span style="color:#ff79c6">=</span> [
</span></span><span style="display:flex;"><span>    {<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;happy&#34;</span>, <span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;sad&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;tall&#34;</span>, <span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;short&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;energetic&#34;</span>, <span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;lethargic&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;sunny&#34;</span>, <span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;gloomy&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;windy&#34;</span>, <span style="color:#f1fa8c">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;calm&#34;</span>},
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 2. 为examples定义example_prompt模板</span>
</span></span><span style="display:flex;"><span>example_prompt <span style="color:#ff79c6">=</span> PromptTemplate(
</span></span><span style="display:flex;"><span>    input_variables<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;input&#34;</span>, <span style="color:#f1fa8c">&#34;output&#34;</span>],
</span></span><span style="display:flex;"><span>    template<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Input: </span><span style="color:#f1fa8c">{input}</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">Output: </span><span style="color:#f1fa8c">{output}</span><span style="color:#f1fa8c">&#34;</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 3. 将example variables list和example_prompt输入到example_selector中</span>
</span></span><span style="display:flex;"><span>example_selector <span style="color:#ff79c6">=</span> LengthBasedExampleSelector(
</span></span><span style="display:flex;"><span>    examples<span style="color:#ff79c6">=</span>examples,
</span></span><span style="display:flex;"><span>    example_prompt<span style="color:#ff79c6">=</span>example_prompt,
</span></span><span style="display:flex;"><span>    max_length<span style="color:#ff79c6">=</span><span style="color:#bd93f9">25</span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 4. 定义few_shot_prompt模板（prefix + examples + suffix）</span>
</span></span><span style="display:flex;"><span>dynamic_prompt <span style="color:#ff79c6">=</span> FewShotPromptTemplate(
</span></span><span style="display:flex;"><span>    example_selector<span style="color:#ff79c6">=</span>example_selector,
</span></span><span style="display:flex;"><span>    example_prompt<span style="color:#ff79c6">=</span>example_prompt,
</span></span><span style="display:flex;"><span>    prefix<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Give the antonym of every input&#34;</span>,  <span style="color:#6272a4"># 前缀，定义task</span>
</span></span><span style="display:flex;"><span>    suffix<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Input: </span><span style="color:#f1fa8c">{adjective}</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">Output:&#34;</span>,  <span style="color:#6272a4"># 后缀</span>
</span></span><span style="display:flex;"><span>    input_variables<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;adjective&#34;</span>],  <span style="color:#6272a4"># 后缀参数变量</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(dynamic_prompt<span style="color:#ff79c6">.</span>invoke({<span style="color:#f1fa8c">&#34;adjective&#34;</span>: <span style="color:#f1fa8c">&#34;funny&#34;</span>})<span style="color:#ff79c6">.</span>text)
</span></span></code></pre></div><p>得到的dynamic_prompt = prefix + examples + suffix如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>Give the antonym of every input
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Input: happy
</span></span><span style="display:flex;"><span>Output: sad
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Input: tall
</span></span><span style="display:flex;"><span>Output: short
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Input: energetic
</span></span><span style="display:flex;"><span>Output: lethargic
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Input: sunny
</span></span><span style="display:flex;"><span>Output: gloomy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Input: windy
</span></span><span style="display:flex;"><span>Output: calm
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Input: funny
</span></span><span style="display:flex;"><span>Output:
</span></span></code></pre></div><h4 id="32模块-language-model">3.2、模块 Language Model</h4>
<p>Language Model是真正与 LLM / ChatModel 进行交互的组件，它可以直接被当作普通的openai client来使用，在LangChain中，主要使用到的是LLM，Chat Model两类Language Model</p>
<p><strong>LLM</strong>: 最基础的通过“text_str in ➡️ text_str out”模式来使用的Language Model，另一方面，LangChain也收录了大量的第三方LLM。【注意：langchain v 0.2.0以后使用langchain_community导入llms模块】</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain.llms <span style="color:#ff79c6">import</span> vllm  <span style="color:#6272a4"># langchain v 0.1.0</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain_community.llms <span style="color:#ff79c6">import</span> vllm  <span style="color:#6272a4"># langchain v 0.2.0</span>
</span></span><span style="display:flex;"><span>llm <span style="color:#ff79c6">=</span> vllm<span style="color:#ff79c6">.</span>VLLM(model<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;/data1/huggingface/LLM/Mistral-7B-Instruct-v0.2&#34;</span>)
</span></span><span style="display:flex;"><span>llm<span style="color:#ff79c6">.</span>invoke({<span style="color:#f1fa8c">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;how can langsmith help with testing?&#34;</span>})
</span></span></code></pre></div><p><strong>Chat Model</strong>: 另一方面，LangChain也收录了大量的第三方Chat Model，是LLM的变体，抽象了Chat这一场景下的使用模式，由“text_str in ➡️ text_str out”变成了“chat_messages in ➡️ chat_message out”。chat_message由2个组件组成：text + message type(System/Human/AI)。【注意：langchain v 0.2.0以后使用langchain_community导入chat_models模块】，3种message type如下：</p>
<ul>
<li>System message - 告诉AI要做什么的背景信息上下文，用于设定llm角色。</li>
<li>Human message - 标识用户传入的消息类型</li>
<li>AI message - 标识AI返回的消息类型</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain_openai <span style="color:#ff79c6">import</span> ChatOpenAI
</span></span><span style="display:flex;"><span>chat <span style="color:#ff79c6">=</span> ChatOpenAI(openai_api_key<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;...&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain_core.messages <span style="color:#ff79c6">import</span> HumanMessage, SystemMessage
</span></span><span style="display:flex;"><span>messages <span style="color:#ff79c6">=</span> [
</span></span><span style="display:flex;"><span>    SystemMessage(content<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;You&#39;re a helpful assistant&#34;</span>),
</span></span><span style="display:flex;"><span>    HumanMessage(content<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;What is the purpose of model regularization?&#34;</span>),
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>chat<span style="color:#ff79c6">.</span>invoke(messages)
</span></span></code></pre></div><h4 id="33模块-output-parsers">3.3、模块 Output Parsers</h4>
<p>langchain.output_parsers中的各种解析器，通常我们希望Language Model的输出是固定的格式，以支持我们解析其输出为结构化数据，LangChain将这一诉求所需的功能抽象成了Output Parser这一组件，并提供了一系列的预定义Output Parser，如最常用的StructuredOutputParser, PydanticOutputParser，JsonOutputParser，以及在LLM输出无法解析时发挥作用的Auto-fixing parser和Retry parser等。</p>
<p><strong>PydanticOutputParser</strong>：PydanticOutputParser可以使用 Pydantic 数据类作为输出格式，将Pydantic数据类导入解析器中。所有数据类都要继承的 Pydantic 的 BaseModel 就像 Python 数据类，但具有实际的类型检查 + 强制。创建prompt时要用partial_variables部分填入变量parser.get_format_instructions()。如下面的例子，生成一问一答的冷笑话（输出的数据类包含问句setup和答句punchline）：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> typing <span style="color:#ff79c6">import</span> List
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain_openai <span style="color:#ff79c6">import</span> ChatOpenAI
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain.output_parsers <span style="color:#ff79c6">import</span> PydanticOutputParser
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> PromptTemplate
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain_core.pydantic_v1 <span style="color:#ff79c6">import</span> BaseModel, Field, validator
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># Define your desired data structure.</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">class</span> <span style="color:#50fa7b">Joke</span>(BaseModel):
</span></span><span style="display:flex;"><span>    setup: <span style="color:#8be9fd;font-style:italic">str</span> <span style="color:#ff79c6">=</span> Field(description<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;question to set up a joke&#34;</span>)  <span style="color:#6272a4"># 笑话设置的部分</span>
</span></span><span style="display:flex;"><span>    punchline: <span style="color:#8be9fd;font-style:italic">str</span> <span style="color:#ff79c6">=</span> Field(description<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;answer to resolve the joke&#34;</span>)  <span style="color:#6272a4"># 笑话的冷笑话部分</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4"># 对笑话进行逻辑验证，保证setup的部分以？结尾</span>
</span></span><span style="display:flex;"><span>    @validator(<span style="color:#f1fa8c">&#34;setup&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">def</span> <span style="color:#50fa7b">question_ends_with_question_mark</span>(cls, field):
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">if</span> field[<span style="color:#ff79c6">-</span><span style="color:#bd93f9">1</span>] <span style="color:#ff79c6">!=</span> <span style="color:#f1fa8c">&#34;?&#34;</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#ff79c6">raise</span> ValueError(<span style="color:#f1fa8c">&#34;Badly formed question!&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">return</span> field
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 初始化ChatOpenAI实例</span>
</span></span><span style="display:flex;"><span>llm <span style="color:#ff79c6">=</span> ChatOpenAI(
</span></span><span style="display:flex;"><span>    model_name<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;qwen1.5-32b-chat-int4&#39;</span>,  <span style="color:#6272a4"># 替换为你使用的模型名称</span>
</span></span><span style="display:flex;"><span>    openai_api_base<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;http://20.20.136.251:8001/v1&#39;</span>,  <span style="color:#6272a4"># 替换为你的API base URL</span>
</span></span><span style="display:flex;"><span>    openai_api_key<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;q7r8s9t0-u1v2-w3x4-y5z6-a7b8c9d0e1f2&#39;</span>  <span style="color:#6272a4"># 替换为你的API密钥</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># And a query intented to prompt a language model to populate the data structure.</span>
</span></span><span style="display:flex;"><span>joke_query <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;Tell me a joke.&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># Set up a parser + inject instructions into the prompt template.</span>
</span></span><span style="display:flex;"><span>parser <span style="color:#ff79c6">=</span> PydanticOutputParser(pydantic_object<span style="color:#ff79c6">=</span>Joke)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#ff79c6">=</span> PromptTemplate(
</span></span><span style="display:flex;"><span>    template<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Answer the user query.</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">{format_instructions}</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">{query}</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>,
</span></span><span style="display:flex;"><span>    input_variables<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;query&#34;</span>],
</span></span><span style="display:flex;"><span>    partial_variables<span style="color:#ff79c6">=</span>{<span style="color:#f1fa8c">&#34;format_instructions&#34;</span>: parser<span style="color:#ff79c6">.</span>get_format_instructions()},
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(prompt)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>chain <span style="color:#ff79c6">=</span> prompt <span style="color:#ff79c6">|</span> llm <span style="color:#ff79c6">|</span> parser
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(chain<span style="color:#ff79c6">.</span>invoke({<span style="color:#f1fa8c">&#34;query&#34;</span>: joke_query}))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>Joke(setup=&#34;Why don&#39;t scientists trust atoms?&#34;, punchline=&#39;Because they make up everything!&#39;)
</span></span></code></pre></div><p><strong>JsonOutputParser</strong>：
必须使用具有足够能力的LLM来生成格式良好的 JSON。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> typing <span style="color:#ff79c6">import</span> List
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain.prompts <span style="color:#ff79c6">import</span> PromptTemplate
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain_core.output_parsers <span style="color:#ff79c6">import</span> JsonOutputParser
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain_core.pydantic_v1 <span style="color:#ff79c6">import</span> BaseModel, Field
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain_openai <span style="color:#ff79c6">import</span> ChatOpenAI
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> ChatOpenAI(temperature<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0</span>)
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># Define your desired data structure.</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">class</span> <span style="color:#50fa7b">Joke</span>(BaseModel):
</span></span><span style="display:flex;"><span>    setup: <span style="color:#8be9fd;font-style:italic">str</span> <span style="color:#ff79c6">=</span> Field(description<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;question to set up a joke&#34;</span>)
</span></span><span style="display:flex;"><span>    punchline: <span style="color:#8be9fd;font-style:italic">str</span> <span style="color:#ff79c6">=</span> Field(description<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;answer to resolve the joke&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4"># And a query intented to prompt a language model to populate the data structure.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>joke_query <span style="color:#ff79c6">=</span> <span style="color:#f1fa8c">&#34;Tell me a joke.&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># Set up a parser + inject instructions into the prompt template.</span>
</span></span><span style="display:flex;"><span>parser <span style="color:#ff79c6">=</span> JsonOutputParser(pydantic_object<span style="color:#ff79c6">=</span>Joke)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#ff79c6">=</span> PromptTemplate(
</span></span><span style="display:flex;"><span>    template<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Answer the user query.</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">{format_instructions}</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">{query}</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">&#34;</span>,
</span></span><span style="display:flex;"><span>    input_variables<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;query&#34;</span>],
</span></span><span style="display:flex;"><span>    partial_variables<span style="color:#ff79c6">=</span>{<span style="color:#f1fa8c">&#34;format_instructions&#34;</span>: parser<span style="color:#ff79c6">.</span>get_format_instructions()},
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>chain <span style="color:#ff79c6">=</span> prompt <span style="color:#ff79c6">|</span> model <span style="color:#ff79c6">|</span> parser
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>chain<span style="color:#ff79c6">.</span>invoke({<span style="color:#f1fa8c">&#34;query&#34;</span>: joke_query})
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>{&#39;setup&#39;: &#34;Why don&#39;t scientists trust atoms?&#34;,
</span></span><span style="display:flex;"><span> &#39;punchline&#39;: &#39;Because they make up everything!&#39;}
</span></span></code></pre></div><p><strong>StructuredOutputParser</strong>：
当您想要返回多个字段时，可以使用此输出解析器。虽然 Pydantic/JSON 解析器更强大，但这个解析器对于生成功能较弱的LLM模型很有用。下面例子是生成问题的答案，并给出参考的web网站url。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain.output_parsers <span style="color:#ff79c6">import</span> ResponseSchema, StructuredOutputParser
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain_core.prompts <span style="color:#ff79c6">import</span> PromptTemplate, load_prompt
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">from</span> langchain_openai <span style="color:#ff79c6">import</span> ChatOpenAI
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 初始化ChatOpenAI实例</span>
</span></span><span style="display:flex;"><span>llm <span style="color:#ff79c6">=</span> ChatOpenAI(
</span></span><span style="display:flex;"><span>    model_name<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;qwen1.5-32b-chat-int4&#39;</span>,  <span style="color:#6272a4"># 替换为你使用的模型名称</span>
</span></span><span style="display:flex;"><span>    openai_api_base<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;http://20.20.136.251:8001/v1&#39;</span>,  <span style="color:#6272a4"># 替换为你的API base URL</span>
</span></span><span style="display:flex;"><span>    openai_api_key<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#39;q7r8s9t0-u1v2-w3x4-y5z6-a7b8c9d0e1f2&#39;</span>  <span style="color:#6272a4"># 替换为你的API密钥</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 响应模式指导LLM的输出解析格式，传入StructuredOutputParser</span>
</span></span><span style="display:flex;"><span>response_schemas <span style="color:#ff79c6">=</span> [
</span></span><span style="display:flex;"><span>    ResponseSchema(name<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;answer&#34;</span>, description<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;answer to the user&#39;s question&#34;</span>),
</span></span><span style="display:flex;"><span>    ResponseSchema(name<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;source&#34;</span>, description<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;source used to answer the user&#39;s question, should be a website.&#34;</span>),
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>output_parser <span style="color:#ff79c6">=</span> StructuredOutputParser<span style="color:#ff79c6">.</span>from_response_schemas(response_schemas)
</span></span><span style="display:flex;"><span>format_instructions <span style="color:#ff79c6">=</span> output_parser<span style="color:#ff79c6">.</span>get_format_instructions()
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(format_instructions)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#ff79c6">=</span> PromptTemplate(
</span></span><span style="display:flex;"><span>    template<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;answer the users question as best as possible.</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">{format_instructions}</span><span style="color:#f1fa8c">\n</span><span style="color:#f1fa8c">{question}</span><span style="color:#f1fa8c">&#34;</span>,
</span></span><span style="display:flex;"><span>    input_variables<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;question&#34;</span>],
</span></span><span style="display:flex;"><span>    partial_variables<span style="color:#ff79c6">=</span>{<span style="color:#f1fa8c">&#34;format_instructions&#34;</span>: format_instructions},
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(prompt)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>chain <span style="color:#ff79c6">=</span> prompt <span style="color:#ff79c6">|</span> llm <span style="color:#ff79c6">|</span> output_parser
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">print</span>(chain<span style="color:#ff79c6">.</span>invoke({<span style="color:#f1fa8c">&#34;question&#34;</span>: <span style="color:#f1fa8c">&#34;what&#39;s the capital of france?&#34;</span>}))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>{&#39;answer&#39;: &#39;The capital of France is Paris.&#39;,
</span></span><span style="display:flex;"><span> &#39;source&#39;: &#39;https://en.wikipedia.org/wiki/Paris&#39;}
</span></span></code></pre></div>

                
                
<div class="entry-shang text-center">
    
	    <p>「真诚赞赏，手留余香」</p>
	
	<button class="zs show-zs btn btn-bred">赞赏支持</button>
</div>
<div class="zs-modal-bg"></div>
<div class="zs-modal-box">
	<div class="zs-modal-head">
		<button type="button" class="close">×</button>
		<span class="author"><a href="https://loveAtCorner.github.io/"><img src="/img/favicon.png" />YuTian Blog</a></span>
        
	        <p class="tip"><i></i><span>真诚赞赏，手留余香</span></p>
		
 
	</div>
	<div class="zs-modal-body">
		<div class="zs-modal-btns">
			<button class="btn btn-blink" data-num="2">2元</button>
			<button class="btn btn-blink" data-num="5">5元</button>
			<button class="btn btn-blink" data-num="10">10元</button>
			<button class="btn btn-blink" data-num="50">50元</button>
			<button class="btn btn-blink" data-num="100">100元</button>
			<button class="btn btn-blink" data-num="1">任意金额</button>
		</div>
		<div class="zs-modal-pay">
			<button class="btn btn-bred" id="pay-text">2元</button>
			<p>使用<span id="pay-type">微信</span>扫描二维码完成支付</p>
			<img src="/img/reward/wechat-2.png"  id="pay-image"/>
		</div>
	</div>
	<div class="zs-modal-footer">
		<label><input type="radio" name="zs-type" value="wechat" class="zs-type" checked="checked"><span ><span class="zs-wechat"><img src="/img/reward/wechat-btn.png"/></span></label>
		<label><input type="radio" name="zs-type" value="alipay" class="zs-type" class="zs-alipay"><img src="/img/reward/alipay-btn.png"/></span></label>
	</div>
</div>
<script type="text/javascript" src="/js/reward.js"></script>

                

                
                <hr>
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2024/08/04/LLMs_agents_tech_detail/" data-toggle="tooltip" data-placement="top" title="Agent技术细节">&larr;
                            Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2024/08/07/LLMs_langchain_practice_agent/" data-toggle="tooltip" data-placement="top" title="langchain开发框架实践（二）">Next
                            Post &rarr;</a>
                    </li>
                    
                </ul>
                

                


            </div>

            
            
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
            

            
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/llms-agent" title="llms, agent">
                            llms, agent
                        </a>
                        
                        
                        
                        <a href="/tags/llms-dev_framework" title="llms, dev_framework">
                            llms, dev_framework
                        </a>
                        
                        
                        
                        <a href="/tags/llmsapi" title="llms,api">
                            llms,api
                        </a>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                    </div>
                </section>
                

                
                
                <section>
                    <hr>
                    <h5>FRIENDS</h5>
                    <ul class="list-inline">
                        
                        <li><a target="_blank" href="https://caixiongjiang.github.io">caixiongjiang的博客</a></li>
                        
                    </ul>
                </section>
                
            </div>
        </div>
    </div>
</article>




<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">                  
                    
                    <li>
                        <a href="mailto:leoyutian@yeah.net">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		           
                    
                    
                    
                    

		            
                    
                    
                    <li>
                        <a target="_blank" href="https://github.com/loveAtCorner">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		            
                    
                    
                    
                    <li>
                        <a target="_blank" href="https://www.linkedin.com/in/tianyu271828">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		           
                    
                    
                    <li>
                        <a target="_blank" href="https://stackoverflow.com/users/yourstackoverflowid">
                            <span class="fa-stack fa-lg">
                                <i class="fas fa-circle fa-stack-2x"></i>
                                <i class="fab fa-stack-overflow fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
            
            
            
           
                   <li>
                       <a href='' rel="alternate" type="application/rss+xml" title="YuTian Blog" >
                           <span class="fa-stack fa-lg">
                               <i class="fas fa-circle fa-stack-2x"></i>
                               <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
                           </span>
                       </a>
                   </li>
            
             </ul>
		<p class="copyright text-muted">
                    Copyright &copy; YuTian Blog 2024
                    
                    <br>
                    <a href="https://themes.gohugo.io/hugo-theme-cleanwhite">CleanWhite Hugo Theme</a> by <a href="https://zhaohuabing.com">Huabing</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true" >
                    </iframe>
                    
                </p>
            </div>
        </div>
    </div>
</footer>




<script>
    function loadAsync(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>






<script>
    
    if($('#tag_cloud').length !== 0){
        loadAsync("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>


<script>
    loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>






<script type="text/javascript">
    function generateCatalog(selector) {

        
        
        
        
            _containerSelector = 'div.post-container'
        

        
        var P = $(_containerSelector), a, n, t, l, i, c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        
        $(selector).html('')

        
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#" + $(this).prop('id');
            t = $(this).text();
            c = $('<a href="' + i + '" rel="nofollow">' + t + '</a>');
            l = $('<li class="' + n + '_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    
    $(".catalog-toggle").click((function (e) {
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    


    loadAsync("\/js\/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>






</body>
</html>
