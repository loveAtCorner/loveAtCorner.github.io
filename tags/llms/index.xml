<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLMs on YuTian Blog</title>
    <link>https://loveAtCorner.github.io/tags/llms/</link>
    <description>Recent content in LLMs on YuTian Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Jun 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://loveAtCorner.github.io/tags/llms/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ollama实操</title>
      <link>https://loveAtCorner.github.io/2024/06/28/ollama/</link>
      <pubDate>Fri, 28 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://loveAtCorner.github.io/2024/06/28/ollama/</guid>
      <description>如何使用ollama在本地启动服务 一. 下载 Ollama 安装文件 访问 https://ollama.com/download ，选择 Windows，单击 “Download for Windows (Preview)” 进行下载。 二.</description>
    </item>
    <item>
      <title>大模型并发接口</title>
      <link>https://loveAtCorner.github.io/2024/06/27/LLMs_asyn_api/</link>
      <pubDate>Thu, 27 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://loveAtCorner.github.io/2024/06/27/LLMs_asyn_api/</guid>
      <description>并发接口开发思路 使用FastAPI编写一个并发接口非常简单，因为FastAPI本身是基于Python的异步框架——Starlette和Pyd</description>
    </item>
    <item>
      <title>大模型接口</title>
      <link>https://loveAtCorner.github.io/2024/06/26/LLMs_api/</link>
      <pubDate>Wed, 26 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://loveAtCorner.github.io/2024/06/26/LLMs_api/</guid>
      <description>开发大模型API接口开发思路 第一步、调用开源模型 开源大模型 from transformers import AutoModelForCausalLM, AutoTokenizer from transformers.generation import GenerationConfig model_path = os.path.join(current_path, &amp;#34;models/Qwen-7B-Chat&amp;#34;) # Model names: &amp;#34;Qwen/Qwen-7B-Chat&amp;#34;, &amp;#34;Qwen/Qwen-14B-Chat&amp;#34; tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True) # use bf16 # model = AutoModelForCausalLM.from_pretrained(model_path, device_map=&amp;#34;auto&amp;#34;, trust_remote_code=True, bf16=True).eval() # use fp16 # model = AutoModelForCausalLM.from_pretrained(model_path, device_map=&amp;#34;auto&amp;#34;, trust_remote_code=True,</description>
    </item>
  </channel>
</rss>
